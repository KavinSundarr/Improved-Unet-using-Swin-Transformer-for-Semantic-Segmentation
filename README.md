
# Aerial-Segmentation-Using-Improved-Unet-using-Swin-Transformer

This project focuses on developing a hybrid UNet model using the Swin Transformer architecture for the segmentation of aerial imagery. The introduction of the Swin Transformer helps capture global details through self-attention mechanisms while maintaining the localization capabilities of the original UNet. The proposed model is evaluated on a multi-class segmentation dataset of satellite images provided by the Mohammed Bin Rashid Space Center, Dubai. Comparative analysis with traditional models like UNet, SegNet, and DeepLabv3+ demonstrates the superior performance of the proposed architecture in producing precise segmentation masks.

**Dataset**  
The dataset for this project can be found on Kaggle:  
[Semantic Segmentation of Aerial Imagery](https://www.kaggle.com/datasets/humansintheloop/semantic-segmentation-of-aerial-imagery)

---

## Architecture Overview

1. **Architecture of the Swin Transformer-based UNet**  
   The diagram below shows the overall architecture of the hybrid UNet, where the convolutional backbone is replaced by the Swin Transformer to enhance global context understanding.

   ![Architecture](https://github.com/user-attachments/assets/d3877958-3b4f-4dfd-a7f4-893e28d3db1a)

2. **Model Pipeline**  
   This image illustrates the data processing pipeline and integration of the Swin Transformer within the UNet framework.

   ![Model Pipeline](https://github.com/user-attachments/assets/e6ce173f-0bfa-4b4d-90b4-05846ad5dd76)

---

## Results

1. **Training Loss & Accuracy**  
   This graph visualizes the training progress of the model, showcasing loss reduction and accuracy improvements over epochs.

   ![Training Results](https://github.com/user-attachments/assets/68b4c632-c48e-429a-bbe0-59152e85613e)

2. **Prediction Masks on Validation Data**  
   Below is a sample of the predicted segmentation masks generated by the Swin Transformer-based UNet on validation data.

   ![Prediction Masks](https://github.com/user-attachments/assets/ed46764c-9b3a-4982-8fb6-6890d65bb0b0)

3. **Ground Truth vs. Prediction Comparison**  
   This comparison highlights how the model's predictions align with the ground truth, indicating the precision and accuracy of the model.

   ![Ground Truth Comparison](https://github.com/user-attachments/assets/8c41e515-ab48-4877-9508-8763f20e3132)

4. **Performance on Test Data**  
   The following image demonstrates the model's performance on test data, showing its robustness across unseen data samples.

   ![Test Data Performance](https://github.com/user-attachments/assets/d167d1f4-05ee-4586-acbc-079df1d41409)

5. **Model Metrics Summary**  
   Here, the precision, recall, F1 score, and overall performance metrics are summarized, reflecting the superiority of the proposed model.

   ![Metrics Summary](https://github.com/user-attachments/assets/43b63e54-ffe1-42bc-972b-ae6a2a98b403)

---

This project highlights the potential of hybrid deep learning architectures, such as the Swin Transformer-based UNet, to outperform traditional segmentation methods, particularly for complex datasets like aerial imagery.

Contributors:
S. Kavin Sundarr
Rohan Inamdar
Dr. Nitish Katal
